For reference, whoever's changing base/file code may occasionally run this benchmark and
update the snapshot below. It can be useful for showing code reviewers the result of a change,
or just so readers can get a sense of performance without running the benchmarks themselves.

Of course, since we're not totally controlling the environment or the data, be careful to
set a baseline before evaluating your change.

Some context for the numbers below:
  * S3 performance guidelines [1] suggest that each request should result in around 85â€“90 MB/s
    of read throughput. Our numbers are MiB/s (not sure if they mean M = 1000^2 or Mi = 1024^2) and
    it looks like our sequential reads ramp up to the right vicinity.
  * EC2 documentation offers network performance expectations (Gbps):
      * m5.x:      1.25 (base) - 10 (burst)
      * m5.4x:     5           - 10
      * m5.12x:   12
      * m5.24x:   25
      * m5n.24x: 100
    Note that a 1000 in the table below is MiB/s which is 8*1.024^2 ~= 8.4 Gbps.

[1] https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html#optimizing-performance-parallelization
[2] https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/general-purpose-instances.html#general-purpose-network-performance

Very brief summary and speculations about our current S3 performance for non-parallel(!) clients:
  * Could be improved for small sequential reads (few MiB at a time) in the current io.Reader
    model by adding (read-ahead) buffering (basically speculating about future reads).
  * Can't get much better for large reads (GiBs) with large buffers (100s MiBs) on small instance
    types because we're getting close to documented network performance.
  * Can probably get better for large reads with large buffers on large, network-optimized (*n)
    instances where we're not especially close to network limits yet. Maybe this requires more
    careful CPU and allocation usage.
Parallel clients might matter more:
  * Clients reading many small files in parallel (like machine learning training reading V3
    fragments file columns) may already do chunked reads pretty effectively. We should measure to
    verify, though; maybe the chunks are not right-sized.
  * Clients reading very large files may benefit even more from a parallel-friendly API (ReaderAt):
      * Copies to disk could be accelerated without huge memory buffers.
        Examples: `grail-file cp`, biofs/gfilefs, reflow interning, bigslice shuffling.
      * Copies to memory could be accelerated without duplicated memory buffers:
        Examples: bio/align/mapper's index (if it was Go), FASTA.

Results:
Rows are read chunk sizes (MiB), columns are number of sequential chunks.
Data cells are average MiB/s for the entire sequential read.

Note: During development, the results seemed worse on some days, peaking around 75 MiB/s rather
than 95 MiB/s. It was consistent for several runs within that day. Unclear if the cause was
something local in the benchmarking machines (or in our builds) or the S3 service.

The numbers below were generated using the bigmachine runner in us-west-2 reading a GRAIL-internal
113 GiB file residing in an S3 bucket in us-west-2. The first columns are reading directly from S3;
the next ones are through FUSE. There was some throttling from S3 (503 SlowDown) which could be
responsible for the oddly slow m5.12x 4096x1 case, for example.

[0] m5.4xlarge
        1       8      64    512    p1      p8      p64     p512
0       0       0      0     4      0       0       0       1
1       6       34     64    55     5       40      226     594
8       41      58     63    73     25      176     872     1058
16      47      58     64           48      247     1041
32      79      78     90           92      321     1093
128     214     250                 270     934
512     380     661                 759     1132
1024    902                         940
4096    1112                        1093

[1] m5.12xlarge
        1       8       64    512    p1      p8      p64     p512
0       0       0       0     4      0       0       0       1
1       7       39      68    80     9       54      377     602
8       47      76      92    72     42      225     838     1308
16      58      77      62           49      435     1209
32      107     107     90           109     638     1209
128     305     254                  253     1301
512     534     1120                 1050    1353
1024    1305                         1285
4096    1348                         1333

[2] m5.24xlarge
        1       8       64     512    p1      p8      p64     p512
0       0       0       1      6      0       0       1       2
1       13      62      84     95     12      97      289     536
8       67      91      93     95     56      459     1785    2303
16      84      87      95            66      468     1678
32      115     133     148           123     858     2559
128     456     452                   493     2158
512     1132    1656                  1480    2668
1024    2431                          2394
4096    2503                          2622

[3] m5n.24xlarge
        1       8       64     512    p1      p8      p64     p512
0       0       0       1      8      0       0       1       2
1       17      70      90     95     16      104     419     801
8       65      86      94     95     48      481     1792    3115
16      80      93      95            83      516     2443
32      151     136     154           163     1058    6245
128     417     596                   473     2903
512     1356    1459                  1692    6501
1024    2359                          2821
4096    4664                          6627